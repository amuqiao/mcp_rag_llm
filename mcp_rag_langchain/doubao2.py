import asyncio
import os
from dotenv import load_dotenv

# -------------------------- LangChain 1.2+ æ ¸å¿ƒæ¨¡å— --------------------------
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# -------------------------- æ–‡æ¡£åŠ è½½ä¸åˆ†å‰² --------------------------
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# -------------------------- å‘é‡å­˜å‚¨ä¸åµŒå…¥ --------------------------
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings

# -------------------------- LLM æ¨¡å‹ --------------------------
from langchain_openai import ChatOpenAI

# -------------------------- MCP å·¥å…·å°è£… --------------------------
from mcp.server.fastmcp import FastMCP

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆ.env æ–‡ä»¶éœ€é…ç½® MODEL/BASE_URL/API_KEY/EMBED_MODELï¼‰
load_dotenv()

class RAGSystem:
    def __init__(self, config):
        self.config = config
        # 1. åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆå…¼å®¹é€šä¹‰åƒé—®ç­‰OpenAIå…¼å®¹æ¥å£ï¼‰
        self.llm = ChatOpenAI(
            model=os.getenv("MODEL", "qwen-plus"),
            base_url=os.getenv("BASE_URL"),
            api_key=os.getenv("API_KEY"),
            temperature=0.1,  # é™ä½éšæœºæ€§ï¼Œä¿è¯å›ç­”å‡†ç¡®æ€§
            max_tokens=2048
        )

        # 2. åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆHuggingFaceä¸­æ–‡åµŒå…¥ï¼‰
        self.embedding = HuggingFaceEmbeddings(
            model_name=os.getenv("EMBED_MODEL", "text2vec-base-chinese"),
            model_kwargs={"device": "cpu"},  # å¯æ”¹ä¸º"cuda"å¯ç”¨GPU
            encode_kwargs={"normalize_embeddings": True}
        )

        # 3. åˆå§‹åŒ–Chromaå‘é‡åº“ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
        self.vectorstore = Chroma(
            collection_name=self.config["collection_name"],
            embedding_function=self.embedding,
            persist_directory=self.config["persist_dir"]
        )

        # 4. åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆMMRç®—æ³•ï¼Œtop_kæ§åˆ¶è¿”å›æ–‡æ¡£æ•°ï¼‰
        self.retriever = self.vectorstore.as_retriever(
            search_type="mmr",  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼Œé¿å…æ–‡æ¡£é‡å¤
            search_kwargs={"k": self.config.get("top_k", 5)}
        )

        # 5. æ„å»ºåŸºäºLCELçš„RAGé“¾ï¼ˆæ ¸å¿ƒæ”¹é€ ç‚¹ï¼‰
        self._build_lcel_rag_chain()

    def _build_lcel_rag_chain(self):
        """æ„å»ºLCELé£æ ¼çš„RAGé“¾ï¼šæ£€ç´¢ â†’ æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ â†’ æç¤ºè¯ â†’ LLM â†’ è§£æè¾“å‡º"""
        # å®šä¹‰ä¸“å±Promptæ¨¡æ¿ï¼ˆé€‚é…æ–—ç ´è‹ç©¹é—®ç­”åœºæ™¯ï¼‰
        prompt = ChatPromptTemplate.from_messages([
            (
                "system",
                "ä½ æ˜¯æ–—ç ´è‹ç©¹å°è¯´çš„ä¸“å±çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œä¸¥æ ¼åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œç¦æ­¢ç¼–é€ ä¿¡æ¯ã€‚"
                "å¦‚æœä¸Šä¸‹æ–‡æ— ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”ã€Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€ã€‚\n\nä¸Šä¸‹æ–‡ï¼š{context}"
            ),
            ("human", "{question}")
        ])

        # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ºå­—ç¬¦ä¸²ï¼ˆä¾›Promptä½¿ç”¨ï¼‰
        def format_documents(docs):
            return "\n\n".join([f"æ–‡æ¡£ç‰‡æ®µï¼š{doc.page_content}" for doc in docs])

        # æ„å»ºLCELé“¾ï¼ˆç®¡é“åŒ–è°ƒç”¨ï¼‰
        self.rag_chain = (
            # å¹¶è¡Œå¤„ç†ï¼šæ£€ç´¢ä¸Šä¸‹æ–‡ + é€ä¼ ç”¨æˆ·é—®é¢˜ + ä¿ç•™æºæ–‡æ¡£
            RunnableParallel({
                "context": self.retriever | format_documents,
                "question": RunnablePassthrough(),
                "source_docs": self.retriever  # ä¿ç•™æºæ–‡æ¡£ç”¨äºè¿”å›æº¯æºä¿¡æ¯
            })
            # ç”Ÿæˆç­”æ¡ˆ + é€ä¼ æºæ–‡æ¡£
            | {
                "answer": prompt | self.llm | StrOutputParser(),  # ç”Ÿæˆå¹¶è§£æå›ç­”
                "source_docs": lambda x: x["source_docs"]  # é€ä¼ æºæ–‡æ¡£ä¿¡æ¯
            }
        )

    def _load_documents(self, file_paths):
        """åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒPDF/TXTï¼‰"""
        docs = []
        for path in file_paths:
            if not os.path.exists(path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}")
            
            if path.endswith(".pdf"):
                loader = PyPDFLoader(path)
            elif path.endswith(".txt"):
                loader = TextLoader(path, encoding="utf-8")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{path}ï¼ˆä»…æ”¯æŒPDF/TXTï¼‰")
            
            docs.extend(loader.load())
        return docs

    def _chunk_documents(self, docs):
        """æ–‡æ¡£åˆ‡å—ï¼ˆé¿å…å•å—è¿‡é•¿ï¼‰"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.get("chunk_size", 500),
            chunk_overlap=self.config.get("chunk_overlap", 50),
            length_function=len,
            is_separator_regex=False  # å…³é—­æ­£åˆ™åˆ†å‰²ï¼Œæå‡å…¼å®¹æ€§
        )
        return text_splitter.split_documents(docs)

    def build_knowledge(self, file_paths):
        """æ„å»ºçŸ¥è¯†åº“ï¼šåŠ è½½æ–‡æ¡£ â†’ åˆ‡å— â†’ å­˜å…¥å‘é‡åº“"""
        # 1. åŠ è½½åŸå§‹æ–‡æ¡£
        raw_docs = self._load_documents(file_paths)
        if not raw_docs:
            print("âš ï¸ æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            return
        
        # 2. æ–‡æ¡£åˆ‡å—
        chunks = self._chunk_documents(raw_docs)
        if not chunks:
            print("âš ï¸ æ–‡æ¡£åˆ‡å—åä¸ºç©º")
            return
        
        # 3. å­˜å…¥å‘é‡åº“å¹¶æŒä¹…åŒ–
        self.vectorstore.add_documents(chunks)
        self.vectorstore.persist()
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…±å­˜å…¥ {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def query(self, question):
        """æ‰§è¡ŒRAGæŸ¥è¯¢ï¼ˆè¿”å›ç­”æ¡ˆ+æºæ–‡æ¡£æº¯æºï¼‰"""
        # è°ƒç”¨LCELé“¾
        result = self.rag_chain.invoke(question)
        
        # æ•´ç†æºæ–‡æ¡£ä¿¡æ¯
        sources = [
            {
                "source": doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶"),
                "page": doc.metadata.get("page", "æ— é¡µç ") if "page" in doc.metadata else "æ— é¡µç "
            }
            for doc in result["source_docs"]
        ]
        
        return {
            "answer": result["answer"],
            "sources": sources
        }

# -------------------------- é…ç½®é¡¹ --------------------------
RAG_CONFIG = {
    "persist_dir": "./data/rag_db",  # å‘é‡åº“æŒä¹…åŒ–ç›®å½•
    "collection_name": "doupocangqiong",  # å‘é‡åº“é›†åˆåï¼ˆæ”¹ä¸ºæ–—ç ´è‹ç©¹ä¸“å±ï¼‰
    "chunk_size": 500,  # æ–‡æ¡£å—å¤§å°
    "chunk_overlap": 50,  # æ–‡æ¡£å—é‡å é•¿åº¦
    "top_k": 5  # æ£€ç´¢è¿”å›æœ€å¤§æ–‡æ¡£æ•°
}

# -------------------------- åˆå§‹åŒ–RAGç³»ç»Ÿ --------------------------
rag = RAGSystem(RAG_CONFIG)

# é¦–æ¬¡è¿è¡Œéœ€æ„å»ºçŸ¥è¯†åº“ï¼ˆæ³¨é‡Šæ‰å·²æ„å»ºçš„æƒ…å†µï¼‰
# rag.build_knowledge(file_paths=["./data/doupocangqiong.txt"])

# -------------------------- MCPå·¥å…·å°è£… --------------------------
mcp = FastMCP("doupocangqiong_rag")

@mcp.tool()
async def rag_query(query: str) -> str:
    """
    æ–—ç ´è‹ç©¹å°è¯´ä¸“å±çŸ¥è¯†æŸ¥è¯¢å·¥å…·
    :param query: ç”¨æˆ·çš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šè§ç‚çš„å¥³æ€§æœ‹å‹æœ‰å“ªäº›ï¼Ÿï¼‰
    :return: åŸºäºå°è¯´åŸæ–‡çš„å‡†ç¡®å›ç­”
    """
    response = rag.query(query)
    return response["answer"]

# -------------------------- æµ‹è¯•Demo --------------------------
async def search_demo():
    """æµ‹è¯•RAGæŸ¥è¯¢åŠŸèƒ½"""
    test_query = "è§ç‚çš„å¥³æ€§æœ‹å‹æœ‰å“ªäº›ï¼Ÿ"
    print(f"ğŸ“ é—®é¢˜ï¼š{test_query}")
    answer = await rag_query(test_query)
    print(f"ğŸ’¡ å›ç­”ï¼š{answer}")

if __name__ == '__main__':
    # è¿è¡Œæµ‹è¯•Demo
    asyncio.run(search_demo())
    
    # å¦‚éœ€å¯åŠ¨MCPæœåŠ¡ï¼ˆé€šè¿‡stdioé€šä¿¡ï¼‰ï¼Œæ³¨é‡Šä¸Šé¢ä¸€è¡Œï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š
    # mcp.run(transport="stdio")